{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Team: &emsp;&emsp; Sasi Kanduri &emsp;&emsp; Vikas Mishra &emsp;&emsp; Ashish Kotian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting .mat files to jpg and image arrays\n",
    "##### The dataset we chose is from the follwoing link which has images in .mat files. We have to take the image arrays out of the .mat constructs and pre-process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import SimpleITK as sitk\n",
    "# List of directories to process\n",
    "directories = [\n",
    "    'brainTumorDataPublic_1-766',\n",
    "    'brainTumorDataPublic_767-1532',\n",
    "    'brainTumorDataPublic_1533-2298',\n",
    "    'brainTumorDataPublic_2299-3064'\n",
    "]\n",
    "\n",
    "y_labels = []\n",
    "filename_column = []\n",
    "\n",
    "image_arrays = []\n",
    "image_classes = []\n",
    "\n",
    "\n",
    "# Set the paths for the Mat and Jpg folders\n",
    "mat_folder = f'./all_files'\n",
    "jpg_folder = f'./Jpg_images'\n",
    "\n",
    "# Create the Jpg folder if it doesn't exist\n",
    "# if not os.path.exists(jpg_folder):\n",
    "#     os.makedirs(jpg_folder)\n",
    "\n",
    "# Iterate through files in the Mat folder\n",
    "for filename in os.listdir(mat_folder):\n",
    "    # Construct the full file paths\n",
    "    mat_filepath = os.path.join(mat_folder, filename)\n",
    "    jpg_filepath = os.path.join(jpg_folder, filename.split(\".\")[0] + '.jpg')\n",
    "\n",
    "    # Open the mat file\n",
    "    with h5py.File(mat_filepath, 'r+') as f:\n",
    "\n",
    "        cjdata = f['cjdata']\n",
    "        image = np.array(cjdata.get('image')).astype(np.float64)\n",
    "        label = cjdata.get('label')[0, 0]\n",
    "        \n",
    "        if image.shape[0] == 512:\n",
    "            \n",
    "            y_labels.append(label - 1)\n",
    "            filename_column.append(filename.split(\".\")[0])\n",
    "\n",
    "            # Perform image processing\n",
    "            hi = np.max(image)\n",
    "            lo = np.min(image)\n",
    "            image = (((image - lo) / (hi - lo)) * 255).astype(np.uint8)\n",
    "\n",
    "            image = cv2.resize(image, (128, 128), interpolation=cv2.INTER_AREA)\n",
    "            image_arrays.append(image)\n",
    "            image_classes.append(label - 1)\n",
    "\n",
    "            # cv2.imwrite(jpg_filepath, image)\n",
    "            # t1_image = sitk.GetImageFromArray(image)\n",
    "            # sitk.WriteImage(t1_image, jpg_filepath)\n",
    "\n",
    "    \n",
    "df = pd.DataFrame({'filename':filename_column, 'label':y_labels})\n",
    "df.to_csv('final_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Embedding,Activation, Dropout\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.utils import plot_model, image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D, Rescaling\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_df.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='filename', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename      int64\n",
       "label       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1], dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['label'].astype('int32')\n",
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_labels = df['label'].to_numpy().tolist()\n",
    "y_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arrays = np.array(image_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arrays = image_arrays.reshape(image_arrays.shape[0], 128, 128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0],\n",
       "        [2],\n",
       "        [5],\n",
       "        ...,\n",
       "        [5],\n",
       "        [2],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [2],\n",
       "        [5],\n",
       "        ...,\n",
       "        [6],\n",
       "        [2],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [2],\n",
       "        [4],\n",
       "        ...,\n",
       "        [6],\n",
       "        [3],\n",
       "        [0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1],\n",
       "        [4],\n",
       "        [6],\n",
       "        ...,\n",
       "        [4],\n",
       "        [3],\n",
       "        [0]],\n",
       "\n",
       "       [[1],\n",
       "        [4],\n",
       "        [6],\n",
       "        ...,\n",
       "        [5],\n",
       "        [2],\n",
       "        [0]],\n",
       "\n",
       "       [[1],\n",
       "        [3],\n",
       "        [6],\n",
       "        ...,\n",
       "        [4],\n",
       "        [2],\n",
       "        [0]]], dtype=uint8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_arrays[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_classes_encoded = tf.keras.utils.to_categorical(image_classes, 3, \"float32\")\n",
    "image_classes_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3049, 128, 128, 1)\n",
      "(3049, 3)\n"
     ]
    }
   ],
   "source": [
    "print(image_arrays.shape)\n",
    "print(image_classes_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(image_arrays, image_classes_encoded, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2286, 128, 128, 1)\n",
      "(763, 128, 128, 1)\n",
      "(2286, 3)\n",
      "(763, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_2 (Rescaling)     (None, 128, 128, 1)       0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 128, 128, 64)      1664      \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPooli  (None, 64, 64, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 64, 64, 64)        16448     \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPooli  (None, 32, 32, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 32, 32, 64)        16448     \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPooli  (None, 16, 16, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 16, 16, 64)        16448     \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPooli  (None, 8, 8, 64)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 8, 8, 64)          16448     \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 4, 4, 64)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 593795 (2.27 MB)\n",
      "Trainable params: 593795 (2.27 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(128, 128, 1)))\n",
    "model.add(Rescaling(1./255)) # scale pixels)\n",
    "#1\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(5, 5), padding='same', strides=1,\n",
    "                 activation='relu')) \n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# 2\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(2, 2), padding='same', strides=1,\n",
    "                 activation='relu')) \n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# 3\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(2, 2), padding='same', strides=1,\n",
    "                 activation='relu')) \n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# 4\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(2, 2), padding='same', strides=1,\n",
    "                 activation='relu')) \n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# 5\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(2, 2), padding='same', strides=1,\n",
    "                 activation='relu')) \n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# flatten and dense\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=\"array_model.hdf5\", verbose=2, save_best_only=True, monitor='val_loss')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 23:48:24.168162: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - ETA: 0s - loss: 0.8676 - accuracy: 0.6203\n",
      "Epoch 1: val_loss improved from inf to 0.72906, saving model to array_model.hdf5\n",
      "72/72 [==============================] - 7s 57ms/step - loss: 0.8676 - accuracy: 0.6203 - val_loss: 0.7291 - val_accuracy: 0.6606\n",
      "Epoch 2/20\n",
      " 3/72 [>.............................] - ETA: 2s - loss: 0.6527 - accuracy: 0.6875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashi/Documents/219-ML/mlenv/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/72 [============================>.] - ETA: 0s - loss: 0.6443 - accuracy: 0.7170\n",
      "Epoch 2: val_loss improved from 0.72906 to 0.60379, saving model to array_model.hdf5\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.6457 - accuracy: 0.7165 - val_loss: 0.6038 - val_accuracy: 0.7274\n",
      "Epoch 3/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.5822 - accuracy: 0.7443\n",
      "Epoch 3: val_loss improved from 0.60379 to 0.59030, saving model to array_model.hdf5\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.5808 - accuracy: 0.7454 - val_loss: 0.5903 - val_accuracy: 0.7130\n",
      "Epoch 4/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.5070 - accuracy: 0.7614\n",
      "Epoch 4: val_loss did not improve from 0.59030\n",
      "72/72 [==============================] - 27s 374ms/step - loss: 0.5063 - accuracy: 0.7620 - val_loss: 0.6010 - val_accuracy: 0.7287\n",
      "Epoch 5/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.4633 - accuracy: 0.7914\n",
      "Epoch 5: val_loss did not improve from 0.59030\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.4667 - accuracy: 0.7900 - val_loss: 0.6131 - val_accuracy: 0.7051\n",
      "Epoch 6/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.4551 - accuracy: 0.8006\n",
      "Epoch 6: val_loss improved from 0.59030 to 0.45429, saving model to array_model.hdf5\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.4559 - accuracy: 0.8001 - val_loss: 0.4543 - val_accuracy: 0.7798\n",
      "Epoch 7/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3955 - accuracy: 0.8314\n",
      "Epoch 7: val_loss did not improve from 0.45429\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.3954 - accuracy: 0.8316 - val_loss: 0.4858 - val_accuracy: 0.7706\n",
      "Epoch 8/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.8341\n",
      "Epoch 8: val_loss improved from 0.45429 to 0.33202, saving model to array_model.hdf5\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.3786 - accuracy: 0.8346 - val_loss: 0.3320 - val_accuracy: 0.8545\n",
      "Epoch 9/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3587 - accuracy: 0.8415\n",
      "Epoch 9: val_loss did not improve from 0.33202\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.3586 - accuracy: 0.8421 - val_loss: 0.3564 - val_accuracy: 0.8388\n",
      "Epoch 10/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3541 - accuracy: 0.8552\n",
      "Epoch 10: val_loss did not improve from 0.33202\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.3553 - accuracy: 0.8543 - val_loss: 0.4394 - val_accuracy: 0.7877\n",
      "Epoch 11/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.8662\n",
      "Epoch 11: val_loss improved from 0.33202 to 0.30541, saving model to array_model.hdf5\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.3238 - accuracy: 0.8666 - val_loss: 0.3054 - val_accuracy: 0.8834\n",
      "Epoch 12/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2883 - accuracy: 0.8829\n",
      "Epoch 12: val_loss improved from 0.30541 to 0.26988, saving model to array_model.hdf5\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.2884 - accuracy: 0.8832 - val_loss: 0.2699 - val_accuracy: 0.9004\n",
      "Epoch 13/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8768\n",
      "Epoch 13: val_loss did not improve from 0.26988\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.2940 - accuracy: 0.8766 - val_loss: 0.2865 - val_accuracy: 0.8755\n",
      "Epoch 14/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2518 - accuracy: 0.8917\n",
      "Epoch 14: val_loss improved from 0.26988 to 0.26393, saving model to array_model.hdf5\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.2518 - accuracy: 0.8915 - val_loss: 0.2639 - val_accuracy: 0.8978\n",
      "Epoch 15/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2451 - accuracy: 0.9102\n",
      "Epoch 15: val_loss improved from 0.26393 to 0.24338, saving model to array_model.hdf5\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.2441 - accuracy: 0.9108 - val_loss: 0.2434 - val_accuracy: 0.9069\n",
      "Epoch 16/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2182 - accuracy: 0.9129\n",
      "Epoch 16: val_loss improved from 0.24338 to 0.23200, saving model to array_model.hdf5\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.2178 - accuracy: 0.9129 - val_loss: 0.2320 - val_accuracy: 0.9017\n",
      "Epoch 17/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2407 - accuracy: 0.9093\n",
      "Epoch 17: val_loss did not improve from 0.23200\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.2403 - accuracy: 0.9094 - val_loss: 0.5022 - val_accuracy: 0.8139\n",
      "Epoch 18/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2290 - accuracy: 0.9129\n",
      "Epoch 18: val_loss did not improve from 0.23200\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.2282 - accuracy: 0.9129 - val_loss: 0.2489 - val_accuracy: 0.9056\n",
      "Epoch 19/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.1962 - accuracy: 0.9278\n",
      "Epoch 19: val_loss did not improve from 0.23200\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.1978 - accuracy: 0.9265 - val_loss: 0.2543 - val_accuracy: 0.9004\n",
      "Epoch 20/20\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2269 - accuracy: 0.9208\n",
      "Epoch 20: val_loss did not improve from 0.23200\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.2265 - accuracy: 0.9208 - val_loss: 0.3623 - val_accuracy: 0.8663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3045686d0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_test, y_test), callbacks=[monitor, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.0274457e-01, 8.9692158e-01, 3.3381802e-04],\n",
       "       [9.3609728e-03, 9.9038213e-01, 2.5684020e-04],\n",
       "       [2.4284853e-02, 3.0051067e-03, 9.7270995e-01],\n",
       "       ...,\n",
       "       [7.7220360e-03, 9.9039090e-01, 1.8870337e-03],\n",
       "       [2.5963894e-04, 2.4598932e-02, 9.7514141e-01],\n",
       "       [1.2172571e-07, 1.7309020e-08, 9.9999988e-01]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"array_model.hdf5\")\n",
    "pred = model.predict(x_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.68      0.78       182\n",
      "           1       0.85      0.97      0.91       354\n",
      "           2       0.99      0.98      0.98       227\n",
      "\n",
      "    accuracy                           0.90       763\n",
      "   macro avg       0.92      0.87      0.89       763\n",
      "weighted avg       0.91      0.90      0.90       763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = np.argmax(y_test, axis=1)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "\n",
    "print(classification_report(y_true, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 ('mlenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b35c18d1f6f2a5a767612809406a6abc2b41160b00d0a4a2fd897ba18e4d655"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
