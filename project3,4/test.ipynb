{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_32 (InputLayer)       [(None, 64, 64, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 61, 61, 64)           3136      ['input_32[0][0]']            \n",
      "                                                                                                  \n",
      " multi_head_attention_22 (M  (None, 61, 61, 64)           62224     ['conv2d_13[0][0]',           \n",
      " ultiHeadAttention)                                                  'conv2d_13[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_13 (Flatten)        (None, 238144)               0         ['multi_head_attention_22[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 128)                  3048256   ['flatten_13[0][0]']          \n",
      "                                                          0                                       \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 10)                   1290      ['dense_12[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 30549210 (116.54 MB)\n",
      "Trainable params: 30549210 (116.54 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, MultiHeadAttention\n",
    "\n",
    "# Assuming you have an input tensor with shape (height, width, channels)\n",
    "input_shape = (64, 64, 3)\n",
    "num_classes = 10\n",
    "\n",
    "# Define the model\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Conv2D(64, (4, 4), activation='relu')(inputs)\n",
    "\n",
    "# Add a self-attention layer\n",
    "attention_output = MultiHeadAttention(key_dim=60, num_heads=4)(x, x)\n",
    "\n",
    "# Flatten the output for further processing\n",
    "flattened = Flatten()(attention_output)\n",
    "\n",
    "# Add more layers as needed\n",
    "x = Dense(128, activation='relu')(flattened)\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model and specify the optimizer, loss function, and metrics\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### team members:\n",
    "### 1. Sasi Kanduri\n",
    "### 2. Vikas Mishra\n",
    "### 3. Ashish Thranath Kotian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Embedding,Activation, Dropout, LSTM, MultiHeadAttention\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model, image_dataset_from_directory\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, Rescaling, MaxPooling2D, concatenate\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing training/validation/test Data from the files downlaoded with image_downloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set x shape : (1921, 2) y shape: (1921,)\n",
      "validate set x shape : (486, 2) y shape: (486,)\n",
      "test set x shape : (477, 2) y shape: (477,)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('final_df_train.csv', index_col=False)\n",
    "df_validate = pd.read_csv('final_df_validate.csv', index_col=False)\n",
    "df_test = pd.read_csv('final_df_test.csv', index_col=False)\n",
    "\n",
    "y_train = df_train['2_way_label'].to_numpy()\n",
    "y_validate = df_validate['2_way_label'].to_numpy()\n",
    "y_test = df_test['2_way_label'].to_numpy()\n",
    "\n",
    "print(f\"train set x shape : {df_train.shape} y shape: {y_train.shape}\")\n",
    "print(f\"validate set x shape : {df_validate.shape} y shape: {y_validate.shape}\")\n",
    "print(f\"test set x shape : {df_test.shape} y shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Model with GLoVe Embedding for text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how does\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\" u \": \" you \",\n",
    "\" ur \": \" your \",\n",
    "\" n \": \" and \"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to clean the text using the contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_text(x):\n",
    "    if type(x) is str:\n",
    "        x = x.lower()\n",
    "        for key in contractions:\n",
    "            value = contractions[key]\n",
    "            x = x.replace(key, value)\n",
    "        return x\n",
    "    else:\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Contraction Function on Train/Validation/Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text with images'] = df_train['text with images'].apply(lambda x: get_clean_text(x))\n",
    "df_validate['text with images'] = df_validate['text with images'].apply(lambda x: get_clean_text(x))\n",
    "df_test['text with images'] = df_test['text with images'].apply(lambda x: get_clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data After Applying Contraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       my walgreens offbrand mucinex was engraved wit...\n",
       "1           hackers leak emails from uae ambassador to us\n",
       "2                                puppy taking in the view\n",
       "3       bride and groom exchange vows after fatal shoo...\n",
       "4                                           major thermos\n",
       "                              ...                        \n",
       "1916              us solar installations hit million mark\n",
       "1917    thought this little flower was pretty interesting\n",
       "1918                          the john rylands library oc\n",
       "1919    this snail who has climbed up to my first stor...\n",
       "1920    plop your infant in front of these pictures of...\n",
       "Name: text with images, Length: 1921, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['text with images']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data After Applying Contraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   my xbox controller says hi\n",
       "1               new image from the mandalorian\n",
       "2                say hello to my little friend\n",
       "3                   watch your step little one\n",
       "4      this tree i found with a solo cup on it\n",
       "                        ...                   \n",
       "481                               high fashion\n",
       "482                    years old world records\n",
       "483                railroad track senior photo\n",
       "484         a rare photograph of billy the kid\n",
       "485        the onion reviews crazy rich asians\n",
       "Name: text with images, Length: 486, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_validate['text with images']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data After Applying Contraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                              stargazer\n",
       "1                                                   yeah\n",
       "2      pd phoenix car thief gets instructions from yo...\n",
       "3      as trump accuses iran he has one problem his o...\n",
       "4                                    believers hezbollah\n",
       "                             ...                        \n",
       "472                                           angry baby\n",
       "473                            this sign in a restaurant\n",
       "474                                       disaster pratt\n",
       "475    reading the manifesto of russia painting by gr...\n",
       "476                             httpsiimgurcomxcvuzmtjpg\n",
       "Name: text with images, Length: 477, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test['text with images']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Tokens using keras tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = df_train['text with images'].tolist()\n",
    "text_validate = df_validate['text with images'].tolist()\n",
    "text_test = df_test['text with images'].tolist()\n",
    "\n",
    "token = Tokenizer()\n",
    "\n",
    "token.fit_on_texts(text_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5063"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_size  = len(token.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Text to Sequences for Train/Validation/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text_train = token.texts_to_sequences(text_train)\n",
    "encoded_text_validate = token.texts_to_sequences(text_validate)\n",
    "encoded_text_test = token.texts_to_sequences(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 225\n",
    "text_train = pad_sequences(encoded_text_train, maxlen=max_length, padding='post')\n",
    "text_validate = pad_sequences(encoded_text_validate, maxlen=max_length, padding='post')\n",
    "text_test = pad_sequences(encoded_text_test, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting data to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = np.array(text_train)\n",
    "text_validate = np.array(text_validate)\n",
    "text_test = np.array(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   8, 1645, 1646,  878,   24, 1647,   13,    1,  879,  878,   90,\n",
       "          6,    2,  250,  323,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Glove Vectors for each word from the pretrained embeddings file 'glove.twitter.27B.25d.txt' - 25 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.21 s, sys: 249 ms, total: 5.46 s\n",
      "Wall time: 5.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "file = open('glove.twitter.27B.25d.txt', encoding='utf-8')\n",
    "\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vectors = np.asarray(values[1: ])\n",
    "    glove_vectors[word] = vectors\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193514"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(glove_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Word Matrix For Each Word In Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_matrix = np.zeros((vocab_size, 25))\n",
    "\n",
    "tokens = []\n",
    "labels = []\n",
    "\n",
    "for word, index in token.word_index.items():   # index returned here starts with 1 so we need set vocab_size = len(token.word_index) + 1  to be able to index up to the greatest token ID\n",
    "    vector = glove_vectors.get(word)\n",
    "    if vector is not None:\n",
    "        word_vector_matrix[index] = vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word matrix size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5063, 25)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_vector_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building The Glove Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text model\n",
    "input1 = Input(shape = (max_length))\n",
    "embedding = Embedding(vocab_size, 25, weights = [word_vector_matrix], trainable = False, name='embedding')(input1)\n",
    "\n",
    "lstm_layer1 = LSTM(units=100)(embedding)\n",
    "\n",
    "dense1 = Dense(64, activation='relu')(lstm_layer1)\n",
    "\n",
    "text_model = Model(inputs = input1, outputs = dense1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model for images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting target label to list for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_labels_train = y_train.tolist()\n",
    "y_labels_validate = y_validate.tolist()\n",
    "y_labels_test = y_test.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset for our train/validation/test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1921 files belonging to 2 classes.\n",
      "Found 486 files belonging to 2 classes.\n",
      "Found 477 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_train = image_dataset_from_directory(\"images_train\", labels=y_labels_train, label_mode=\"binary\", image_size=(64,64), batch_size=32, color_mode='rgb')\n",
    "image_val = image_dataset_from_directory(\"images_validate\", labels=y_labels_validate, label_mode=\"binary\", image_size=(64,64), batch_size=32, color_mode='rgb')\n",
    "image_test = image_dataset_from_directory(\"images_test\", labels=y_labels_test, label_mode=\"binary\", image_size=(64,64), batch_size=32, color_mode='rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autotune with keras for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = image_train.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# train data\n",
    "data_batches = []\n",
    "\n",
    "for batch in train_dataset:\n",
    "    data_batches.append(batch[0])\n",
    "\n",
    "image_data_train = np.concatenate(data_batches, axis=0)\n",
    "\n",
    "#validation data\n",
    "val_dataset = image_val.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "data_batches = []\n",
    "\n",
    "for batch in val_dataset:\n",
    "    try:\n",
    "        data_batches.append(batch[0])\n",
    "    except:\n",
    "        print(batch)\n",
    "\n",
    "image_data_validate = np.concatenate(data_batches, axis=0)\n",
    "\n",
    "# testd data\n",
    "test_dataset = image_test.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "data_batches = []\n",
    "\n",
    "for batch in test_dataset:\n",
    "    data_batches.append(batch[0])\n",
    "\n",
    "image_data_test = np.concatenate(data_batches, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1921, 64, 64, 3)\n",
      "(486, 64, 64, 3)\n",
      "(477, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(image_data_train.shape)\n",
    "print(image_data_validate.shape)\n",
    "print(image_data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input2 = Input(shape=(64,64,3))\n",
    "rescaling = Rescaling(1./255)(input2) # scale pixels\n",
    "\n",
    "conv1 = Conv2D(100, (4, 4), activation='relu')(rescaling)\n",
    "pool1 = MaxPooling2D((2, 2), padding='same')(conv1)\n",
    "conv2 = Conv2D(64, (2, 2), activation='relu')(pool1)\n",
    "pool2 = MaxPooling2D((2, 2), padding='same')(conv2)\n",
    "\n",
    "flat_layer = Flatten()(pool2)\n",
    "dense2 = Dense(64, activation='relu')(flat_layer)\n",
    "\n",
    "image_model = Model(inputs = input2, outputs = dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a multimodal architecture by merging the dense layers of text and image models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_15 (InputLayer)       [(None, 225)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 225, 25)           126575    \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 100)               50400     \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 64)                6464      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183439 (716.56 KB)\n",
      "Trainable params: 56864 (222.12 KB)\n",
      "Non-trainable params: 126575 (494.43 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " rescaling_7 (Rescaling)     (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 61, 61, 100)       4900      \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 31, 31, 100)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 30, 30, 64)        25664     \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPooli  (None, 15, 15, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 14400)             0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 64)                921664    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 952228 (3.63 MB)\n",
      "Trainable params: 952228 (3.63 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)       [(None, 64, 64, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " rescaling_7 (Rescaling)     (None, 64, 64, 3)            0         ['input_16[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 61, 61, 100)          4900      ['rescaling_7[0][0]']         \n",
      "                                                                                                  \n",
      " max_pooling2d_14 (MaxPooli  (None, 31, 31, 100)          0         ['conv2d_14[0][0]']           \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " input_15 (InputLayer)       [(None, 225)]                0         []                            \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 30, 30, 64)           25664     ['max_pooling2d_14[0][0]']    \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 225, 25)              126575    ['input_15[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_15 (MaxPooli  (None, 15, 15, 64)           0         ['conv2d_15[0][0]']           \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " lstm_8 (LSTM)               (None, 100)                  50400     ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)         (None, 14400)                0         ['max_pooling2d_15[0][0]']    \n",
      "                                                                                                  \n",
      " dense_44 (Dense)            (None, 64)                   6464      ['lstm_8[0][0]']              \n",
      "                                                                                                  \n",
      " dense_45 (Dense)            (None, 64)                   921664    ['flatten_7[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate  (None, 128)                  0         ['dense_44[0][0]',            \n",
      " )                                                                   'dense_45[0][0]']            \n",
      "                                                                                                  \n",
      " dense_46 (Dense)            (None, 128)                  16512     ['concatenate_9[0][0]']       \n",
      "                                                                                                  \n",
      " dense_47 (Dense)            (None, 32)                   4128      ['dense_46[0][0]']            \n",
      "                                                                                                  \n",
      " dense_48 (Dense)            (None, 1)                    33        ['dense_47[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1156340 (4.41 MB)\n",
      "Trainable params: 1029765 (3.93 MB)\n",
      "Non-trainable params: 126575 (494.43 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "merge = concatenate([dense1, dense2])\n",
    "\n",
    "hidden1 = Dense(128, activation='relu')(merge)\n",
    "hidden2 = Dense(32, activation='relu')(hidden1)\n",
    "output = Dense(1, activation='sigmoid')(hidden2)\n",
    "\n",
    "base_model = Model(inputs=[input1, input2], outputs=output)\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8342 - accuracy: 0.5544\n",
      "Epoch 1: val_loss improved from inf to 0.68774, saving model to best_nultimodal.hdf5\n",
      "61/61 [==============================] - 23s 243ms/step - loss: 0.8342 - accuracy: 0.5544 - val_loss: 0.6877 - val_accuracy: 0.5700\n",
      "Epoch 2/50\n",
      " 3/61 [>.............................] - ETA: 2s - loss: 0.7184 - accuracy: 0.4271"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashi/Documents/219-ML/mlenv/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - ETA: 0s - loss: 0.6852 - accuracy: 0.5659\n",
      "Epoch 2: val_loss improved from 0.68774 to 0.67463, saving model to best_nultimodal.hdf5\n",
      "61/61 [==============================] - 3s 46ms/step - loss: 0.6852 - accuracy: 0.5659 - val_loss: 0.6746 - val_accuracy: 0.5988\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6755 - accuracy: 0.5960\n",
      "Epoch 3: val_loss did not improve from 0.67463\n",
      "61/61 [==============================] - 3s 45ms/step - loss: 0.6755 - accuracy: 0.5960 - val_loss: 0.6787 - val_accuracy: 0.5988\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6796 - accuracy: 0.5971\n",
      "Epoch 4: val_loss improved from 0.67463 to 0.67362, saving model to best_nultimodal.hdf5\n",
      "61/61 [==============================] - 3s 44ms/step - loss: 0.6796 - accuracy: 0.5971 - val_loss: 0.6736 - val_accuracy: 0.5988\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6802 - accuracy: 0.5966\n",
      "Epoch 5: val_loss did not improve from 0.67362\n",
      "61/61 [==============================] - 3s 43ms/step - loss: 0.6802 - accuracy: 0.5966 - val_loss: 0.6805 - val_accuracy: 0.5905\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6854 - accuracy: 0.5325\n",
      "Epoch 6: val_loss did not improve from 0.67362\n",
      "61/61 [==============================] - 3s 44ms/step - loss: 0.6854 - accuracy: 0.5325 - val_loss: 0.6757 - val_accuracy: 0.5967\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6915 - accuracy: 0.5508\n",
      "Epoch 7: val_loss did not improve from 0.67362\n",
      "61/61 [==============================] - 3s 42ms/step - loss: 0.6915 - accuracy: 0.5508 - val_loss: 0.6754 - val_accuracy: 0.5967\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6910 - accuracy: 0.5653\n",
      "Epoch 8: val_loss did not improve from 0.67362\n",
      "61/61 [==============================] - 3s 44ms/step - loss: 0.6910 - accuracy: 0.5653 - val_loss: 0.7315 - val_accuracy: 0.4177\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7001 - accuracy: 0.5534\n",
      "Epoch 9: val_loss did not improve from 0.67362\n",
      "61/61 [==============================] - 3s 42ms/step - loss: 0.7001 - accuracy: 0.5534 - val_loss: 0.6915 - val_accuracy: 0.5988\n",
      "Epoch 9: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x5d126d450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=\"best_nultimodal.hdf5\", verbose=2, save_best_only=True, monitor='val_loss')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')\n",
    "\n",
    "base_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "base_model.fit([text_train, image_data_train], y_train, epochs=50, callbacks=[monitor, checkpointer], batch_size = 32,\n",
    "            validation_data=([text_validate, image_data_validate], y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 3s 86ms/step\n"
     ]
    }
   ],
   "source": [
    "base_model.load_weights(\"best_nultimodal.hdf5\")\n",
    "pred = base_model.predict([text_test, image_data_test])\n",
    "pred\n",
    "pred = (pred > 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      1.00      0.74       278\n",
      "           1       0.00      0.00      0.00       199\n",
      "\n",
      "    accuracy                           0.58       477\n",
      "   macro avg       0.29      0.50      0.37       477\n",
      "weighted avg       0.34      0.58      0.43       477\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashi/Documents/219-ML/mlenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/shashi/Documents/219-ML/mlenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/shashi/Documents/219-ML/mlenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_34 (InputLayer)       [(None, 64, 64, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 62, 62, 64)           1792      ['input_34[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 3844, 64)             0         ['conv2d_15[0][0]']           \n",
      "                                                                                                  \n",
      " multi_head_attention_24 (M  (None, 3844, 64)             66368     ['reshape_1[0][0]',           \n",
      " ultiHeadAttention)                                                  'reshape_1[0][0]',           \n",
      "                                                                     'reshape_1[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_14 (Flatten)        (None, 246016)               0         ['multi_head_attention_24[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 128)                  3149017   ['flatten_14[0][0]']          \n",
      "                                                          6                                       \n",
      "                                                                                                  \n",
      " dense_15 (Dense)            (None, 10)                   1290      ['dense_14[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31559626 (120.39 MB)\n",
      "Trainable params: 31559626 (120.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MultiHeadAttention, Flatten, Dense\n",
    "\n",
    "# Create input layer\n",
    "input_layer = Input(shape=(64, 64, 3))\n",
    "\n",
    "# Convolutional layer\n",
    "conv_layer = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(input_layer)\n",
    "\n",
    "# Reshape the output of the convolutional layer\n",
    "reshape_layer = tf.keras.layers.Reshape((-1, 64))(conv_layer)\n",
    "\n",
    "# Multi-headed self-attention layer\n",
    "attention_layer = MultiHeadAttention(key_dim=64, num_heads=4)(reshape_layer, reshape_layer, reshape_layer)\n",
    "\n",
    "# Flatten the attention output\n",
    "flatten_layer = Flatten()(attention_layer)\n",
    "\n",
    "# Fully connected layer\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model and specify the optimizer, loss function, and metrics\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
