{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project 4\n",
        "### team members:\n",
        "### 1. Sasi Kanduri &emsp; 2. Vikas Mishra &emsp;3. Ashish Thranath Kotian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Base Model from project 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Flatten,Embedding,Activation, Dropout, LSTM, MultiHeadAttention\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Attention\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model, image_dataset_from_directory\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, Rescaling, MaxPooling2D, concatenate\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing training/validation/test Data from the files downlaoded with image_downloader.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train set x shape : (1921, 2) y shape: (1921,)\n",
            "validate set x shape : (486, 2) y shape: (486,)\n",
            "test set x shape : (477, 2) y shape: (477,)\n"
          ]
        }
      ],
      "source": [
        "df_train = pd.read_csv('final_df_train.csv', index_col=False)\n",
        "df_validate = pd.read_csv('final_df_validate.csv', index_col=False)\n",
        "df_test = pd.read_csv('final_df_test.csv', index_col=False)\n",
        "\n",
        "y_train = df_train['2_way_label'].to_numpy()\n",
        "y_validate = df_validate['2_way_label'].to_numpy()\n",
        "y_test = df_test['2_way_label'].to_numpy()\n",
        "\n",
        "print(f\"train set x shape : {df_train.shape} y shape: {y_train.shape}\")\n",
        "print(f\"validate set x shape : {df_validate.shape} y shape: {y_validate.shape}\")\n",
        "print(f\"test set x shape : {df_test.shape} y shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Model with GLoVe Embedding for text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [],
      "source": [
        "contractions = {\n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how does\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so is\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\" u \": \" you \",\n",
        "\" ur \": \" your \",\n",
        "\" n \": \" and \"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function to clean the text using the contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_clean_text(x):\n",
        "    if type(x) is str:\n",
        "        x = x.lower()\n",
        "        for key in contractions:\n",
        "            value = contractions[key]\n",
        "            x = x.replace(key, value)\n",
        "        return x\n",
        "    else:\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Applying Contraction Function on Train/Validation/Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train['text with images'] = df_train['text with images'].apply(lambda x: get_clean_text(x))\n",
        "df_validate['text with images'] = df_validate['text with images'].apply(lambda x: get_clean_text(x))\n",
        "df_test['text with images'] = df_test['text with images'].apply(lambda x: get_clean_text(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Data After Applying Contraction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       my walgreens offbrand mucinex was engraved wit...\n",
              "1           hackers leak emails from uae ambassador to us\n",
              "2                                puppy taking in the view\n",
              "3       bride and groom exchange vows after fatal shoo...\n",
              "4                                           major thermos\n",
              "                              ...                        \n",
              "1916              us solar installations hit million mark\n",
              "1917    thought this little flower was pretty interesting\n",
              "1918                          the john rylands library oc\n",
              "1919    this snail who has climbed up to my first stor...\n",
              "1920    plop your infant in front of these pictures of...\n",
              "Name: text with images, Length: 1921, dtype: object"
            ]
          },
          "execution_count": 183,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train['text with images']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation Data After Applying Contraction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                   my xbox controller says hi\n",
              "1               new image from the mandalorian\n",
              "2                say hello to my little friend\n",
              "3                   watch your step little one\n",
              "4      this tree i found with a solo cup on it\n",
              "                        ...                   \n",
              "481                               high fashion\n",
              "482                    years old world records\n",
              "483                railroad track senior photo\n",
              "484         a rare photograph of billy the kid\n",
              "485        the onion reviews crazy rich asians\n",
              "Name: text with images, Length: 486, dtype: object"
            ]
          },
          "execution_count": 184,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_validate['text with images']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Data After Applying Contraction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                                              stargazer\n",
              "1                                                   yeah\n",
              "2      pd phoenix car thief gets instructions from yo...\n",
              "3      as trump accuses iran he has one problem his o...\n",
              "4                                    believers hezbollah\n",
              "                             ...                        \n",
              "472                                           angry baby\n",
              "473                            this sign in a restaurant\n",
              "474                                       disaster pratt\n",
              "475    reading the manifesto of russia painting by gr...\n",
              "476                             httpsiimgurcomxcvuzmtjpg\n",
              "Name: text with images, Length: 477, dtype: object"
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test['text with images']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Tokens using keras tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_train = df_train['text with images'].tolist()\n",
        "text_validate = df_validate['text with images'].tolist()\n",
        "text_test = df_test['text with images'].tolist()\n",
        "\n",
        "token = Tokenizer()\n",
        "\n",
        "token.fit_on_texts(text_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### vocabulary size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5063"
            ]
          },
          "execution_count": 187,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size  = len(token.word_index) + 1\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding Text to Sequences for Train/Validation/Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoded_text_train = token.texts_to_sequences(text_train)\n",
        "encoded_text_validate = token.texts_to_sequences(text_validate)\n",
        "encoded_text_test = token.texts_to_sequences(text_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_length = 225\n",
        "text_train = pad_sequences(encoded_text_train, maxlen=max_length, padding='post')\n",
        "text_validate = pad_sequences(encoded_text_validate, maxlen=max_length, padding='post')\n",
        "text_test = pad_sequences(encoded_text_test, maxlen=max_length, padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Converting data to numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_train = np.array(text_train)\n",
        "text_validate = np.array(text_validate)\n",
        "text_test = np.array(text_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reading the Glove Vectors for each word from the pretrained embeddings file 'glove.twitter.27B.25d.txt' - 25 dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [],
      "source": [
        "glove_vectors = dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 4.66 s, sys: 207 ms, total: 4.86 s\n",
            "Wall time: 4.86 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "file = open('glove.twitter.27B.25d.txt', encoding='utf-8')\n",
        "\n",
        "for line in file:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vectors = np.asarray(values[1: ])\n",
        "    glove_vectors[word] = vectors\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1193514"
            ]
          },
          "execution_count": 193,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(glove_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Word Matrix For Each Word In Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_vector_matrix = np.zeros((vocab_size, 25))\n",
        "\n",
        "tokens = []\n",
        "labels = []\n",
        "\n",
        "for word, index in token.word_index.items():   # index returned here starts with 1 so we need set vocab_size = len(token.word_index) + 1  to be able to index up to the greatest token ID\n",
        "    vector = glove_vectors.get(word)\n",
        "    if vector is not None:\n",
        "        word_vector_matrix[index] = vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Word matrix size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5063, 25)"
            ]
          },
          "execution_count": 195,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_vector_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building The Glove Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [],
      "source": [
        "# text model\n",
        "input1 = Input(shape = (max_length))\n",
        "embedding = Embedding(vocab_size, 25, weights = [word_vector_matrix], trainable = False, name='embedding')(input1)\n",
        "\n",
        "layer1 = Conv1D(64, 8, activation = 'relu')(embedding)\n",
        "layer2 = MaxPooling1D(2)(layer1)\n",
        "layer3 = Conv1D(32, 4, activation = 'relu')(layer2)\n",
        "layer4 = MaxPooling1D(2)(layer3)\n",
        "\n",
        "flat1 = Flatten()(layer4)\n",
        "dense1 = Dense(128, activation='relu')(flat1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CNN model for images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Converting target label to list for labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "y_labels_train = y_train.tolist()\n",
        "y_labels_validate = y_validate.tolist()\n",
        "y_labels_test = y_test.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating dataset for our train/validation/test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1921 files belonging to 2 classes.\n",
            "Found 486 files belonging to 2 classes.\n",
            "Found 477 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "image_train = image_dataset_from_directory(\"images_train\", labels=y_labels_train, label_mode=\"binary\", image_size=(64,64), batch_size=32, color_mode='rgb')\n",
        "image_val = image_dataset_from_directory(\"images_validate\", labels=y_labels_validate, label_mode=\"binary\", image_size=(64,64), batch_size=32, color_mode='rgb')\n",
        "image_test = image_dataset_from_directory(\"images_test\", labels=y_labels_test, label_mode=\"binary\", image_size=(64,64), batch_size=32, color_mode='rgb')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Autotune with keras for performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {},
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_dataset = image_train.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# train data\n",
        "data_batches = []\n",
        "\n",
        "for batch in train_dataset:\n",
        "    data_batches.append(batch[0])\n",
        "\n",
        "image_data_train = np.concatenate(data_batches, axis=0)\n",
        "\n",
        "#validation data\n",
        "val_dataset = image_val.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "data_batches = []\n",
        "\n",
        "for batch in val_dataset:\n",
        "    try:\n",
        "        data_batches.append(batch[0])\n",
        "    except:\n",
        "        print(batch)\n",
        "\n",
        "image_data_validate = np.concatenate(data_batches, axis=0)\n",
        "\n",
        "# testd data\n",
        "test_dataset = image_test.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "data_batches = []\n",
        "\n",
        "for batch in test_dataset:\n",
        "    data_batches.append(batch[0])\n",
        "\n",
        "image_data_test = np.concatenate(data_batches, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1921, 64, 64, 3)\n",
            "(486, 64, 64, 3)\n",
            "(477, 64, 64, 3)\n"
          ]
        }
      ],
      "source": [
        "print(image_data_train.shape)\n",
        "print(image_data_validate.shape)\n",
        "print(image_data_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building the CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {},
      "outputs": [],
      "source": [
        "input2 = Input(shape=(64,64,3))\n",
        "rescaling = Rescaling(1./255)(input2) # scale pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {},
      "outputs": [],
      "source": [
        "input2 = Input(shape=(64,64,3))\n",
        "rescaling = Rescaling(1./255)(input2) # scale pixels\n",
        "\n",
        "conv1 = Conv2D(100, (4, 4), activation='relu')(rescaling)\n",
        "pool1 = MaxPooling2D((2, 2), padding='same')(conv1)\n",
        "conv2 = Conv2D(64, (2, 2), activation='relu')(pool1)\n",
        "pool2 = MaxPooling2D((2, 2), padding='same')(conv2)\n",
        "\n",
        "flat_layer = Flatten()(pool2)\n",
        "dense2 = Dense(64, activation='relu')(flat_layer)\n",
        "\n",
        "image_model = Model(inputs = input2, outputs = dense2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a multimodal architecture by merging the dense layers of text and image models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_20\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)       [(None, 225)]                0         []                            \n",
            "                                                                                                  \n",
            " input_15 (InputLayer)       [(None, 64, 64, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 225, 25)              126575    ['input_13[0][0]']            \n",
            "                                                                                                  \n",
            " rescaling_9 (Rescaling)     (None, 64, 64, 3)            0         ['input_15[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)           (None, 218, 64)              12864     ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 61, 61, 100)          4900      ['rescaling_9[0][0]']         \n",
            "                                                                                                  \n",
            " max_pooling1d_8 (MaxPoolin  (None, 109, 64)              0         ['conv1d_8[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " max_pooling2d_8 (MaxPoolin  (None, 31, 31, 100)          0         ['conv2d_8[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)           (None, 106, 32)              8224      ['max_pooling1d_8[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 30, 30, 64)           25664     ['max_pooling2d_8[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling1d_9 (MaxPoolin  (None, 53, 32)               0         ['conv1d_9[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " max_pooling2d_9 (MaxPoolin  (None, 15, 15, 64)           0         ['conv2d_9[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " flatten_8 (Flatten)         (None, 1696)                 0         ['max_pooling1d_9[0][0]']     \n",
            "                                                                                                  \n",
            " flatten_9 (Flatten)         (None, 14400)                0         ['max_pooling2d_9[0][0]']     \n",
            "                                                                                                  \n",
            " dense_53 (Dense)            (None, 128)                  217216    ['flatten_8[0][0]']           \n",
            "                                                                                                  \n",
            " dense_54 (Dense)            (None, 64)                   921664    ['flatten_9[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenat  (None, 192)                  0         ['dense_53[0][0]',            \n",
            " e)                                                                  'dense_54[0][0]']            \n",
            "                                                                                                  \n",
            " dense_55 (Dense)            (None, 128)                  24704     ['concatenate_10[0][0]']      \n",
            "                                                                                                  \n",
            " dense_56 (Dense)            (None, 32)                   4128      ['dense_55[0][0]']            \n",
            "                                                                                                  \n",
            " dense_57 (Dense)            (None, 1)                    33        ['dense_56[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1345972 (5.13 MB)\n",
            "Trainable params: 1219397 (4.65 MB)\n",
            "Non-trainable params: 126575 (494.43 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "merge = concatenate([dense1, dense2])\n",
        "\n",
        "hidden1 = Dense(128, activation='relu')(merge)\n",
        "hidden2 = Dense(32, activation='relu')(hidden1)\n",
        "output = Dense(1, activation='sigmoid')(hidden2)\n",
        "\n",
        "model = Model(inputs=[input1, input2], outputs=output)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining ModelCheckPoint & EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6954 - accuracy: 0.5997\n",
            "Epoch 1: val_loss improved from inf to 0.68312, saving model to best_nultimodal.hdf5\n",
            "61/61 [==============================] - 16s 162ms/step - loss: 0.6954 - accuracy: 0.5997 - val_loss: 0.6831 - val_accuracy: 0.6317\n",
            "Epoch 2/50\n",
            " 5/61 [=>............................] - ETA: 1s - loss: 0.6304 - accuracy: 0.7000"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/shashi/Documents/219-ML/mlenv/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "61/61 [==============================] - ETA: 0s - loss: 0.5807 - accuracy: 0.6965\n",
            "Epoch 2: val_loss improved from 0.68312 to 0.61762, saving model to best_nultimodal.hdf5\n",
            "61/61 [==============================] - 2s 27ms/step - loss: 0.5807 - accuracy: 0.6965 - val_loss: 0.6176 - val_accuracy: 0.6667\n",
            "Epoch 3/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.5759 - accuracy: 0.6955\n",
            "Epoch 3: val_loss did not improve from 0.61762\n",
            "61/61 [==============================] - 1s 24ms/step - loss: 0.5759 - accuracy: 0.6955 - val_loss: 0.6422 - val_accuracy: 0.6831\n",
            "Epoch 4/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.5857 - accuracy: 0.7054\n",
            "Epoch 4: val_loss did not improve from 0.61762\n",
            "61/61 [==============================] - 2s 26ms/step - loss: 0.5857 - accuracy: 0.7054 - val_loss: 0.7173 - val_accuracy: 0.6523\n",
            "Epoch 5/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8048 - accuracy: 0.6486\n",
            "Epoch 5: val_loss did not improve from 0.61762\n",
            "61/61 [==============================] - 2s 26ms/step - loss: 0.8048 - accuracy: 0.6486 - val_loss: 3.7045 - val_accuracy: 0.5988\n",
            "Epoch 6/50\n",
            "60/61 [============================>.] - ETA: 0s - loss: 1.0093 - accuracy: 0.5958\n",
            "Epoch 6: val_loss did not improve from 0.61762\n",
            "61/61 [==============================] - 1s 24ms/step - loss: 1.0088 - accuracy: 0.5960 - val_loss: 0.7489 - val_accuracy: 0.6440\n",
            "Epoch 7/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6075 - accuracy: 0.6668\n",
            "Epoch 7: val_loss did not improve from 0.61762\n",
            "61/61 [==============================] - 2s 24ms/step - loss: 0.6075 - accuracy: 0.6668 - val_loss: 0.9077 - val_accuracy: 0.6337\n",
            "Epoch 7: early stopping\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x36dec1990>"
            ]
          },
          "execution_count": 204,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpointer = ModelCheckpoint(filepath=\"best_nultimodal.hdf5\", verbose=2, save_best_only=True, monitor='val_loss')\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "model.fit([text_train, image_data_train], y_train, epochs=50, callbacks=[monitor, checkpointer], batch_size = 32,\n",
        "            validation_data=([text_validate, image_data_validate], y_validate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predicting with test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 2s 77ms/step\n"
          ]
        }
      ],
      "source": [
        "model.load_weights(\"best_nultimodal.hdf5\")\n",
        "pred = model.predict([text_test, image_data_test])\n",
        "\n",
        "pred = (pred > 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.72      0.73       278\n",
            "           1       0.63      0.66      0.64       199\n",
            "\n",
            "    accuracy                           0.69       477\n",
            "   macro avg       0.69      0.69      0.69       477\n",
            "weighted avg       0.70      0.69      0.70       477\n",
            "\n"
          ]
        }
      ],
      "source": [
        "base_mode_result = classification_report(y_test, pred)\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building LSTM for text model and reshaping the dimensions of visual features for attention layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {},
      "outputs": [],
      "source": [
        "# image model output\n",
        "conv_output_reshaped = tf.keras.layers.Reshape((-1, 64), name='image_model')(pool2)\n",
        "\n",
        "# text model output\n",
        "lstm_layer1 = LSTM(units=100, return_sequences=True)(embedding)\n",
        "lstm_layer2 = LSTM(units=64, return_sequences=True, name=\"for_attention\")(lstm_layer1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building cross attention layer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_21\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_15 (InputLayer)       [(None, 64, 64, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " rescaling_9 (Rescaling)     (None, 64, 64, 3)            0         ['input_15[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 61, 61, 100)          4900      ['rescaling_9[0][0]']         \n",
            "                                                                                                  \n",
            " max_pooling2d_8 (MaxPoolin  (None, 31, 31, 100)          0         ['conv2d_8[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " input_13 (InputLayer)       [(None, 225)]                0         []                            \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 30, 30, 64)           25664     ['max_pooling2d_8[0][0]']     \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 225, 25)              126575    ['input_13[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling2d_9 (MaxPoolin  (None, 15, 15, 64)           0         ['conv2d_9[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)               (None, 225, 100)             50400     ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " image_model (Reshape)       (None, 225, 64)              0         ['max_pooling2d_9[0][0]']     \n",
            "                                                                                                  \n",
            " for_attention (LSTM)        (None, 225, 64)              42240     ['lstm_3[0][0]']              \n",
            "                                                                                                  \n",
            " multi_head_attention_14 (M  (None, 225, 64)              132672    ['image_model[0][0]',         \n",
            " ultiHeadAttention)                                                  'for_attention[0][0]']       \n",
            "                                                                                                  \n",
            " tf.__operators__.add_11 (T  (None, 225, 64)              0         ['multi_head_attention_14[0][0\n",
            " FOpLambda)                                                         ]',                           \n",
            "                                                                     'for_attention[0][0]']       \n",
            "                                                                                                  \n",
            " layer_normalization_27 (La  (None, 225, 64)              128       ['tf.__operators__.add_11[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1  (None, 64)                   0         ['layer_normalization_27[0][0]\n",
            " 1 (GlobalAveragePooling1D)                                         ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_28 (La  (None, 64)                   128       ['global_average_pooling1d_11[\n",
            " yerNormalization)                                                  0][0]']                       \n",
            "                                                                                                  \n",
            " dense_58 (Dense)            (None, 64)                   4160      ['layer_normalization_28[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_59 (Dense)            (None, 32)                   2080      ['dense_58[0][0]']            \n",
            "                                                                                                  \n",
            " dense_60 (Dense)            (None, 1)                    33        ['dense_59[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 388980 (1.48 MB)\n",
            "Trainable params: 262405 (1.00 MB)\n",
            "Non-trainable params: 126575 (494.43 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "cross_attention_layer = MultiHeadAttention(key_dim=64, num_heads=8)(conv_output_reshaped, lstm_layer2)\n",
        "\n",
        "add_norm = layers.LayerNormalization(epsilon=1e-6)(cross_attention_layer + lstm_layer2)\n",
        "\n",
        "pool = layers.GlobalAveragePooling1D()(add_norm)\n",
        "\n",
        "out1 = layers.LayerNormalization(epsilon=1e-6)(pool)\n",
        "\n",
        "ffn1 = Dense(64, activation='relu')(out1)\n",
        "ffn2 = Dense(32, activation='relu')(ffn1)\n",
        "\n",
        "final = Dense(1, activation='sigmoid')(ffn2)\n",
        "\n",
        "\n",
        "cross_model = Model(inputs = [input1, input2], outputs = final)\n",
        "cross_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7740 - accuracy: 0.5846\n",
            "Epoch 1: val_loss improved from inf to 0.66058, saving model to cross_attention_model.hdf5\n",
            "61/61 [==============================] - 50s 549ms/step - loss: 0.7740 - accuracy: 0.5846 - val_loss: 0.6606 - val_accuracy: 0.6193\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/shashi/Documents/219-ML/mlenv/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "61/61 [==============================] - ETA: 0s - loss: 0.6949 - accuracy: 0.5471\n",
            "Epoch 2: val_loss did not improve from 0.66058\n",
            "61/61 [==============================] - 15s 246ms/step - loss: 0.6949 - accuracy: 0.5471 - val_loss: 0.6923 - val_accuracy: 0.5988\n",
            "Epoch 3/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6899 - accuracy: 0.5711\n",
            "Epoch 3: val_loss did not improve from 0.66058\n",
            "61/61 [==============================] - 15s 241ms/step - loss: 0.6899 - accuracy: 0.5711 - val_loss: 0.6689 - val_accuracy: 0.5988\n",
            "Epoch 4/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6831 - accuracy: 0.5679\n",
            "Epoch 4: val_loss improved from 0.66058 to 0.65361, saving model to cross_attention_model.hdf5\n",
            "61/61 [==============================] - 15s 242ms/step - loss: 0.6831 - accuracy: 0.5679 - val_loss: 0.6536 - val_accuracy: 0.5988\n",
            "Epoch 5/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6227 - accuracy: 0.6158\n",
            "Epoch 5: val_loss improved from 0.65361 to 0.63092, saving model to cross_attention_model.hdf5\n",
            "61/61 [==============================] - 127s 2s/step - loss: 0.6227 - accuracy: 0.6158 - val_loss: 0.6309 - val_accuracy: 0.6790\n",
            "Epoch 6/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.5842 - accuracy: 0.6356\n",
            "Epoch 6: val_loss improved from 0.63092 to 0.62097, saving model to cross_attention_model.hdf5\n",
            "61/61 [==============================] - 15s 243ms/step - loss: 0.5842 - accuracy: 0.6356 - val_loss: 0.6210 - val_accuracy: 0.6543\n",
            "Epoch 7/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.5607 - accuracy: 0.6793\n",
            "Epoch 7: val_loss improved from 0.62097 to 0.61050, saving model to cross_attention_model.hdf5\n",
            "61/61 [==============================] - 15s 238ms/step - loss: 0.5607 - accuracy: 0.6793 - val_loss: 0.6105 - val_accuracy: 0.5988\n",
            "Epoch 8/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6084 - accuracy: 0.5976\n",
            "Epoch 8: val_loss did not improve from 0.61050\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 0.6084 - accuracy: 0.5976 - val_loss: 0.6676 - val_accuracy: 0.5988\n",
            "Epoch 9/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.5914 - accuracy: 0.6523\n",
            "Epoch 9: val_loss did not improve from 0.61050\n",
            "61/61 [==============================] - 15s 240ms/step - loss: 0.5914 - accuracy: 0.6523 - val_loss: 0.6131 - val_accuracy: 0.6914\n",
            "Epoch 10/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.5621 - accuracy: 0.6543\n",
            "Epoch 10: val_loss did not improve from 0.61050\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 0.5621 - accuracy: 0.6543 - val_loss: 0.6439 - val_accuracy: 0.6317\n",
            "Epoch 11/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.5619 - accuracy: 0.6512\n",
            "Epoch 11: val_loss did not improve from 0.61050\n",
            "61/61 [==============================] - 14s 238ms/step - loss: 0.5619 - accuracy: 0.6512 - val_loss: 0.7016 - val_accuracy: 0.6975\n",
            "Epoch 12/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.5827 - accuracy: 0.6153\n",
            "Epoch 12: val_loss did not improve from 0.61050\n",
            "61/61 [==============================] - 14s 238ms/step - loss: 0.5827 - accuracy: 0.6153 - val_loss: 0.6167 - val_accuracy: 0.6728\n",
            "Epoch 12: early stopping\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x6fc268dd0>"
            ]
          },
          "execution_count": 209,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checkpointer = ModelCheckpoint(filepath=\"cross_attention_model.hdf5\", verbose=2, save_best_only=True, monitor='val_loss')\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')\n",
        "\n",
        "cross_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "cross_model.fit([text_train, image_data_train], y_train, epochs=50, callbacks=[monitor],\n",
        "            validation_data=([text_validate, image_data_validate], y_validate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "wpWCOSg2CZxq",
        "outputId": "6a6e4ef9-482a-449b-9064-ecccf778d390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 8s 280ms/step\n"
          ]
        }
      ],
      "source": [
        "pred = cross_model.predict([text_test, image_data_test])\n",
        "\n",
        "pred = (pred > 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "fwUO7G7uCZxq",
        "outputId": "0de65e4d-a401-4d2f-f41c-545cf099bec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      1.00      0.74       278\n",
            "           1       0.00      0.00      0.00       199\n",
            "\n",
            "    accuracy                           0.58       477\n",
            "   macro avg       0.29      0.50      0.37       477\n",
            "weighted avg       0.34      0.58      0.43       477\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/shashi/Documents/219-ML/mlenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/shashi/Documents/219-ML/mlenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/shashi/Documents/219-ML/mlenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/shashi/Documents/219-ML/mlenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/shashi/Documents/219-ML/mlenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Users/shashi/Documents/219-ML/mlenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "cross_model_results = classification_report(y_test, pred)\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building self attention layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_features = concatenate([conv_output_reshaped, lstm_layer2], name=\"merged\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_22\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_15 (InputLayer)       [(None, 64, 64, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " rescaling_9 (Rescaling)     (None, 64, 64, 3)            0         ['input_15[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 61, 61, 100)          4900      ['rescaling_9[0][0]']         \n",
            "                                                                                                  \n",
            " max_pooling2d_8 (MaxPoolin  (None, 31, 31, 100)          0         ['conv2d_8[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " input_13 (InputLayer)       [(None, 225)]                0         []                            \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 30, 30, 64)           25664     ['max_pooling2d_8[0][0]']     \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 225, 25)              126575    ['input_13[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling2d_9 (MaxPoolin  (None, 15, 15, 64)           0         ['conv2d_9[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)               (None, 225, 100)             50400     ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " image_model (Reshape)       (None, 225, 64)              0         ['max_pooling2d_9[0][0]']     \n",
            "                                                                                                  \n",
            " for_attention (LSTM)        (None, 225, 64)              42240     ['lstm_3[0][0]']              \n",
            "                                                                                                  \n",
            " merged (Concatenate)        (None, 225, 128)             0         ['image_model[0][0]',         \n",
            "                                                                     'for_attention[0][0]']       \n",
            "                                                                                                  \n",
            " multi_head_attention_15 (M  (None, 225, 128)             263808    ['merged[0][0]',              \n",
            " ultiHeadAttention)                                                  'merged[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_12 (T  (None, 225, 128)             0         ['multi_head_attention_15[0][0\n",
            " FOpLambda)                                                         ]',                           \n",
            "                                                                     'merged[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_29 (La  (None, 225, 128)             256       ['tf.__operators__.add_12[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1  (None, 128)                  0         ['layer_normalization_29[0][0]\n",
            " 2 (GlobalAveragePooling1D)                                         ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_30 (La  (None, 128)                  256       ['global_average_pooling1d_12[\n",
            " yerNormalization)                                                  0][0]']                       \n",
            "                                                                                                  \n",
            " dense_61 (Dense)            (None, 64)                   8256      ['layer_normalization_30[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_62 (Dense)            (None, 32)                   2080      ['dense_61[0][0]']            \n",
            "                                                                                                  \n",
            " dense_63 (Dense)            (None, 1)                    33        ['dense_62[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 524468 (2.00 MB)\n",
            "Trainable params: 397893 (1.52 MB)\n",
            "Non-trainable params: 126575 (494.43 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "self_attention = MultiHeadAttention(key_dim=64, num_heads=8)(combined_features, combined_features)\n",
        "\n",
        "add_norm = layers.LayerNormalization(epsilon=1e-6)(self_attention + combined_features)\n",
        "\n",
        "pool = layers.GlobalAveragePooling1D()(add_norm)\n",
        "\n",
        "out1 = layers.LayerNormalization(epsilon=1e-6)(pool)\n",
        "\n",
        "ffn1 = Dense(64, activation='relu')(out1)\n",
        "ffn2 = Dense(32, activation='relu')(ffn1)\n",
        "\n",
        "final = Dense(1, activation='sigmoid')(ffn2)\n",
        "\n",
        "self_attention_model = Model(inputs = [input1, input2], outputs = final)\n",
        "self_attention_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6491 - accuracy: 0.6330\n",
            "Epoch 1: val_loss improved from inf to 0.64728, saving model to self_attention_model.hdf5\n",
            "61/61 [==============================] - 57s 611ms/step - loss: 0.6491 - accuracy: 0.6330 - val_loss: 0.6473 - val_accuracy: 0.5967\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/shashi/Documents/219-ML/mlenv/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "61/61 [==============================] - ETA: 0s - loss: 0.6117 - accuracy: 0.5945\n",
            "Epoch 2: val_loss did not improve from 0.64728\n",
            "61/61 [==============================] - 16s 264ms/step - loss: 0.6117 - accuracy: 0.5945 - val_loss: 0.6550 - val_accuracy: 0.5638\n",
            "Epoch 3/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6641 - accuracy: 0.5632\n",
            "Epoch 3: val_loss did not improve from 0.64728\n",
            "61/61 [==============================] - 16s 257ms/step - loss: 0.6641 - accuracy: 0.5632 - val_loss: 0.6711 - val_accuracy: 0.5988\n",
            "Epoch 4/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7028 - accuracy: 0.5549\n",
            "Epoch 4: val_loss did not improve from 0.64728\n",
            "61/61 [==============================] - 15s 253ms/step - loss: 0.7028 - accuracy: 0.5549 - val_loss: 0.7027 - val_accuracy: 0.5926\n",
            "Epoch 5/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7110 - accuracy: 0.5377\n",
            "Epoch 5: val_loss did not improve from 0.64728\n",
            "61/61 [==============================] - 16s 255ms/step - loss: 0.7110 - accuracy: 0.5377 - val_loss: 0.6906 - val_accuracy: 0.5988\n",
            "Epoch 6/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6579 - accuracy: 0.5799\n",
            "Epoch 6: val_loss did not improve from 0.64728\n",
            "61/61 [==============================] - 15s 252ms/step - loss: 0.6579 - accuracy: 0.5799 - val_loss: 0.6572 - val_accuracy: 0.5988\n",
            "Epoch 6: early stopping\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x6ebbebc50>"
            ]
          },
          "execution_count": 214,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checkpointer = ModelCheckpoint(filepath=\"self_attention_model.hdf5\", verbose=2, save_best_only=True, monitor='val_loss')\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')\n",
        "\n",
        "self_attention_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "self_attention_model.fit([text_train, image_data_train], y_train, epochs=50, callbacks=[monitor],\n",
        "            validation_data=([text_validate, image_data_validate], y_validate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 8s 293ms/step\n"
          ]
        }
      ],
      "source": [
        "# self_attention_model.load_weights(\"self_attention_model.hdf5\")\n",
        "pred = self_attention_model.predict([text_test, image_data_test])\n",
        "\n",
        "pred = (pred > 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.99      0.73       278\n",
            "           1       0.56      0.03      0.05       199\n",
            "\n",
            "    accuracy                           0.58       477\n",
            "   macro avg       0.57      0.51      0.39       477\n",
            "weighted avg       0.57      0.58      0.45       477\n",
            "\n"
          ]
        }
      ],
      "source": [
        "self_attention_model_results = classification_report(y_test, pred)\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building co-attention layer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_23\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_15 (InputLayer)       [(None, 64, 64, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " rescaling_9 (Rescaling)     (None, 64, 64, 3)            0         ['input_15[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 61, 61, 100)          4900      ['rescaling_9[0][0]']         \n",
            "                                                                                                  \n",
            " max_pooling2d_8 (MaxPoolin  (None, 31, 31, 100)          0         ['conv2d_8[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " input_13 (InputLayer)       [(None, 225)]                0         []                            \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 30, 30, 64)           25664     ['max_pooling2d_8[0][0]']     \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 225, 25)              126575    ['input_13[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling2d_9 (MaxPoolin  (None, 15, 15, 64)           0         ['conv2d_9[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)               (None, 225, 100)             50400     ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " image_model (Reshape)       (None, 225, 64)              0         ['max_pooling2d_9[0][0]']     \n",
            "                                                                                                  \n",
            " for_attention (LSTM)        (None, 225, 64)              42240     ['lstm_3[0][0]']              \n",
            "                                                                                                  \n",
            " multi_head_attention_16 (M  (None, 225, 64)              132672    ['image_model[0][0]',         \n",
            " ultiHeadAttention)                                                  'for_attention[0][0]']       \n",
            "                                                                                                  \n",
            " multi_head_attention_17 (M  (None, 225, 64)              132672    ['for_attention[0][0]',       \n",
            " ultiHeadAttention)                                                  'image_model[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenat  (None, 225, 128)             0         ['multi_head_attention_16[0][0\n",
            " e)                                                                 ]',                           \n",
            "                                                                     'multi_head_attention_17[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenat  (None, 225, 128)             0         ['image_model[0][0]',         \n",
            " e)                                                                  'for_attention[0][0]']       \n",
            "                                                                                                  \n",
            " tf.__operators__.add_13 (T  (None, 225, 128)             0         ['concatenate_11[0][0]',      \n",
            " FOpLambda)                                                          'concatenate_12[0][0]']      \n",
            "                                                                                                  \n",
            " layer_normalization_31 (La  (None, 225, 128)             256       ['tf.__operators__.add_13[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1  (None, 128)                  0         ['layer_normalization_31[0][0]\n",
            " 3 (GlobalAveragePooling1D)                                         ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_32 (La  (None, 128)                  256       ['global_average_pooling1d_13[\n",
            " yerNormalization)                                                  0][0]']                       \n",
            "                                                                                                  \n",
            " dense_64 (Dense)            (None, 64)                   8256      ['layer_normalization_32[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_65 (Dense)            (None, 32)                   2080      ['dense_64[0][0]']            \n",
            "                                                                                                  \n",
            " dense_66 (Dense)            (None, 1)                    33        ['dense_65[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 526004 (2.01 MB)\n",
            "Trainable params: 399429 (1.52 MB)\n",
            "Non-trainable params: 126575 (494.43 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "co_attention_layer1 = MultiHeadAttention(key_dim=64, num_heads=8)(conv_output_reshaped, lstm_layer2)\n",
        "co_attention_layer2 = MultiHeadAttention(key_dim=64, num_heads=8)(lstm_layer2, conv_output_reshaped)\n",
        "\n",
        "fused = concatenate([co_attention_layer1, co_attention_layer2])\n",
        "\n",
        "add_norm = layers.LayerNormalization(epsilon=1e-6)(fused + concatenate([conv_output_reshaped, lstm_layer2]))\n",
        "\n",
        "pool = layers.GlobalAveragePooling1D()(add_norm)\n",
        "\n",
        "out1 = layers.LayerNormalization(epsilon=1e-6)(pool)\n",
        "\n",
        "ffn1 = Dense(64, activation='relu')(out1)\n",
        "ffn2 = Dense(32, activation='relu')(ffn1)\n",
        "\n",
        "final = Dense(1, activation='sigmoid')(ffn2)\n",
        "\n",
        "co_attention_model = Model(inputs = [input1, input2], outputs = final)\n",
        "co_attention_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6419 - accuracy: 0.6143\n",
            "Epoch 1: val_loss improved from inf to 0.70117, saving model to co_attention_model.hdf5\n",
            "61/61 [==============================] - 66s 752ms/step - loss: 0.6419 - accuracy: 0.6143 - val_loss: 0.7012 - val_accuracy: 0.6276\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/shashi/Documents/219-ML/mlenv/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "41/61 [===================>..........] - ETA: 5s - loss: 0.6004 - accuracy: 0.6265"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/shashi/Documents/219-ML/project4.ipynb Cell 75\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shashi/Documents/219-ML/project4.ipynb#Y134sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m monitor \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, min_delta\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shashi/Documents/219-ML/project4.ipynb#Y134sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m co_attention_model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39madam\u001b[39m\u001b[39m\"\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m\"\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/shashi/Documents/219-ML/project4.ipynb#Y134sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m co_attention_model\u001b[39m.\u001b[39;49mfit([text_train, image_data_train], y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[monitor, checkpointer],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shashi/Documents/219-ML/project4.ipynb#Y134sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m             validation_data\u001b[39m=\u001b[39;49m([text_validate, image_data_validate], y_validate))\n",
            "File \u001b[0;32m~/Documents/219-ML/mlenv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/Documents/219-ML/mlenv/lib/python3.11/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m~/Documents/219-ML/mlenv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/Documents/219-ML/mlenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/Documents/219-ML/mlenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[1;32m    869\u001b[0m   )\n\u001b[1;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[0;32m~/Documents/219-ML/mlenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/219-ML/mlenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mflat_call(args)\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[0;32m~/Documents/219-ML/mlenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[0;32m~/Documents/219-ML/mlenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    253\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    254\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    255\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
            "File \u001b[0;32m~/Documents/219-ML/mlenv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1480\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1485\u001b[0m   )\n\u001b[1;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
            "File \u001b[0;32m~/Documents/219-ML/mlenv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# checkpointer = ModelCheckpoint(filepath=\"co_attention_model.hdf5\", verbose=2, save_best_only=True, monitor='val_loss')\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')\n",
        "\n",
        "co_attention_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "co_attention_model.fit([text_train, image_data_train], y_train, epochs=50, callbacks=[monitor],\n",
        "            validation_data=([text_validate, image_data_validate], y_validate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# co_attention_model.load_weights(\"co_attention_model.hdf5\")\n",
        "pred = co_attention_model.predict([text_test, image_data_test])\n",
        "\n",
        "pred = (pred > 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "co_attention_model_results = classification_report(y_test, pred)\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "print(\"================================= No Attention ======================================\")\n",
        "print(base_mode_result)\n",
        "print(\"================================= Cross Attention ====================================\")\n",
        "print(cross_model_results)\n",
        "print(\"================================= Self Attention =====================================\")\n",
        "print(self_attention_model_results)\n",
        "print(\"================================= Co Attention  ======================================\")\n",
        "print(co_attention_model_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VGG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "# Loading VGG16 model\n",
        "\n",
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_tensor=input2)\n",
        "base_model.trainable = False\n",
        "\n",
        "vgg_model = Sequential()\n",
        "\n",
        "for x in base_model.layers[:4]:\n",
        "    vgg_model.add(x)\n",
        "\n",
        "vgg_last = vgg_model.layers[-1].output\n",
        "conv1 = Conv2D(100, (4, 4), activation='relu')(vgg_last)\n",
        "pool1 = MaxPooling2D((2, 2), padding='same')(conv1)\n",
        "conv2 = Conv2D(64, (2, 2), activation='relu')(pool1)\n",
        "pool2 = MaxPooling2D((2, 2), padding='same')(conv2)\n",
        "\n",
        "vgg_output_reshaped = tf.keras.layers.Reshape((-1, 64), name='layer_taken_out_for_attentions')(pool2)\n",
        "\n",
        "sample_model = Model(inputs = vgg_model.layers[0].output, outputs=vgg_output_reshaped)\n",
        "\n",
        "sample_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building cross attention layer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "cross_attention_layer = MultiHeadAttention(key_dim=64, num_heads=8)(vgg_output_reshaped, lstm_layer2)\n",
        "\n",
        "pool = layers.GlobalAveragePooling1D()(cross_attention_layer)\n",
        "\n",
        "out1 = layers.LayerNormalization(epsilon=1e-6)(pool)\n",
        "\n",
        "ffn_output = Sequential(\n",
        "            [layers.Dense(64, activation=\"relu\"), layers.Dense(32)]\n",
        "        )(out1)\n",
        "\n",
        "out2 = layers.LayerNormalization(epsilon=1e-6)(ffn_output)\n",
        "\n",
        "final = Dense(1, activation='sigmoid')(out2)\n",
        "\n",
        "cross_model = Model(inputs = [input1, input2], outputs = final)\n",
        "cross_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpointer = ModelCheckpoint(filepath=\"cross_attention_model_vgg.hdf5\", verbose=2, save_best_only=True, monitor='val_loss')\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')\n",
        "\n",
        "cross_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "cross_model.fit([text_train, image_data_train], y_train, epochs=50, callbacks=[monitor, checkpointer],\n",
        "            validation_data=([text_validate, image_data_validate], y_validate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cross_model.load_weights(\"cross_attention_model_vgg.hdf5\")\n",
        "pred = cross_model.predict([text_test, image_data_test])\n",
        "\n",
        "pred = (pred > 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classification_report(y_test, pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.16 ('csc219')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "46063a162fd018717c54ebe31a63604c323942fa1bfed7ede4efd019204c37b5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
