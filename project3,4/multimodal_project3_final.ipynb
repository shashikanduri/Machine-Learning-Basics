{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project 3 - Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### team members:\n",
        "### 1. Sasi Kanduri\n",
        "### 2. Vikas Mishra\n",
        "### 3. Ashish Thranath Kotian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 496,
      "metadata": {
        "id": "Bbaxwqh7OqgW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Flatten,Embedding,Activation, Dropout\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model, image_dataset_from_directory\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D, Rescaling\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import concatenate\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghBmcYm4CZxh"
      },
      "source": [
        "### Importing training/validation/test Data from the files downlaoded with image_downloader.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 497,
      "metadata": {
        "id": "5He_dbyOCZxh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train set x shape : (4812, 2) y shape: (4812,)\n",
            "validate set x shape : (968, 2) y shape: (968,)\n",
            "test set x shape : (963, 2) y shape: (963,)\n"
          ]
        }
      ],
      "source": [
        "df_train = pd.read_csv('final_df_train.csv', index_col=False)\n",
        "df_validate = pd.read_csv('final_df_validate.csv', index_col=False)\n",
        "df_test = pd.read_csv('final_df_test.csv', index_col=False)\n",
        "\n",
        "y_train = df_train['2_way_label'].to_numpy()\n",
        "y_validate = df_validate['2_way_label'].to_numpy()\n",
        "y_test = df_test['2_way_label'].to_numpy()\n",
        "\n",
        "print(f\"train set x shape : {df_train.shape} y shape: {y_train.shape}\")\n",
        "print(f\"validate set x shape : {df_validate.shape} y shape: {y_validate.shape}\")\n",
        "print(f\"test set x shape : {df_test.shape} y shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Model with GLoVe Embedding for text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYaQTsfwCZxh"
      },
      "source": [
        "### Contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 498,
      "metadata": {
        "id": "MjTvGoW9OqgY"
      },
      "outputs": [],
      "source": [
        "contractions = {\n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how does\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so is\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\" u \": \" you \",\n",
        "\" ur \": \" your \",\n",
        "\" n \": \" and \"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTaJWnZhCZxi"
      },
      "source": [
        "### Function to clean the text using the contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 499,
      "metadata": {
        "id": "HkFIhmCxQ8kS"
      },
      "outputs": [],
      "source": [
        "def get_clean_text(x):\n",
        "    if type(x) is str:\n",
        "        x = x.lower()\n",
        "        for key in contractions:\n",
        "            value = contractions[key]\n",
        "            x = x.replace(key, value)\n",
        "        return x\n",
        "    else:\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkBT3RAbCZxi"
      },
      "source": [
        "### Applying Contraction Function on Train/Validation/Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 500,
      "metadata": {
        "id": "TAU39ns2CZxi"
      },
      "outputs": [],
      "source": [
        "df_train['text with images'] = df_train['text with images'].apply(lambda x: get_clean_text(x))\n",
        "df_validate['text with images'] = df_validate['text with images'].apply(lambda x: get_clean_text(x))\n",
        "df_test['text with images'] = df_test['text with images'].apply(lambda x: get_clean_text(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OETmV1qiCZxj"
      },
      "source": [
        "### Train Data After Applying Contraction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 501,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1ax3naSRd-D",
        "outputId": "c18ae884-4796-45c6-eb85-2c2002e9c4d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       my walgreens offbrand mucinex was engraved wit...\n",
              "1           hackers leak emails from uae ambassador to us\n",
              "2                                puppy taking in the view\n",
              "3       bride and groom exchange vows after fatal shoo...\n",
              "4                                           major thermos\n",
              "                              ...                        \n",
              "4807    old lady cupboard smiles at you over the rims ...\n",
              "4808          video how to stay motivated during workdays\n",
              "4809                             the scourge of carpathia\n",
              "4810    las vegas shooting multiple casualties near ma...\n",
              "4811    a year quest to find the brother he never knew...\n",
              "Name: text with images, Length: 4812, dtype: object"
            ]
          },
          "execution_count": 501,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train['text with images']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlJZ1zUeCZxj"
      },
      "source": [
        "### Validation Data After Applying Contraction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 502,
      "metadata": {
        "id": "OilYHnosCZxj",
        "outputId": "7573c25c-9683-47c8-ac3e-fbf0096150bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                             my xbox controller says hi\n",
              "1                         new image from the mandalorian\n",
              "2                          say hello to my little friend\n",
              "3                             watch your step little one\n",
              "4                this tree i found with a solo cup on it\n",
              "                             ...                        \n",
              "963    a rare picture of walter cronkite reacting to ...\n",
              "964    i cant win till the dank memes have been beate...\n",
              "965                               heres a two headed dog\n",
              "966    victim of the pirate francois lolonnais having...\n",
              "967                                       available here\n",
              "Name: text with images, Length: 968, dtype: object"
            ]
          },
          "execution_count": 502,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_validate['text with images']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJxU5JflCZxk"
      },
      "source": [
        "### Test Data After Applying Contraction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 503,
      "metadata": {
        "id": "L70D5RDBCZxk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                                              stargazer\n",
              "1                                                   yeah\n",
              "2      pd phoenix car thief gets instructions from yo...\n",
              "3      as trump accuses iran he has one problem his o...\n",
              "4                                    believers hezbollah\n",
              "                             ...                        \n",
              "958                       buy domestic japan advertising\n",
              "959          a spider crawled into my taillight and died\n",
              "960        no buyers for hell michigan despite fire sale\n",
              "961                                     he had it coming\n",
              "962       im on a steve buscemi kick for whatever reason\n",
              "Name: text with images, Length: 963, dtype: object"
            ]
          },
          "execution_count": 503,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test['text with images']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSL5CrBYCZxk"
      },
      "source": [
        "### Generating Tokens using keras tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 504,
      "metadata": {
        "id": "pLGB4l6FOqgZ"
      },
      "outputs": [],
      "source": [
        "text_train = df_train['text with images'].tolist()\n",
        "text_validate = df_validate['text with images'].tolist()\n",
        "text_test = df_test['text with images'].tolist()\n",
        "\n",
        "token = Tokenizer()\n",
        "\n",
        "token.fit_on_texts(text_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnotrP1ECZxk"
      },
      "source": [
        "### vocabulary size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 505,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qie8NBehOqga",
        "outputId": "cb7ed26b-f540-4e02-8528-0a527b443e68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9246"
            ]
          },
          "execution_count": 505,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size  = len(token.word_index) + 1\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2YROC2OCZxl"
      },
      "source": [
        "### Encoding Text to Sequences for Train/Validation/Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 506,
      "metadata": {
        "id": "B0R6J6YrOqga"
      },
      "outputs": [],
      "source": [
        "encoded_text_train = token.texts_to_sequences(text_train)\n",
        "encoded_text_validate = token.texts_to_sequences(text_validate)\n",
        "encoded_text_test = token.texts_to_sequences(text_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9t-tmnmCZxl"
      },
      "source": [
        "### Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 507,
      "metadata": {
        "id": "XZISAt-eOqga"
      },
      "outputs": [],
      "source": [
        "max_length = 120\n",
        "text_train = pad_sequences(encoded_text_train, maxlen=max_length, padding='post')\n",
        "text_validate = pad_sequences(encoded_text_validate, maxlen=max_length, padding='post')\n",
        "text_test = pad_sequences(encoded_text_test, maxlen=max_length, padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An6eFIMyCZxl"
      },
      "source": [
        "### Converting data to numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 508,
      "metadata": {
        "id": "2oIwQ08bOqgb"
      },
      "outputs": [],
      "source": [
        "text_train = np.array(text_train)\n",
        "text_validate = np.array(text_validate)\n",
        "text_test = np.array(text_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlEeTY7DCZxm"
      },
      "source": [
        "### Reading the Glove Vectors for each word from the pretrained embeddings file 'glove.twitter.27B.25d.txt' - 25 dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 509,
      "metadata": {
        "id": "weK3uD5jT94h"
      },
      "outputs": [],
      "source": [
        "glove_vectors = dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 510,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhgGuWhHT_lS",
        "outputId": "d191e9b1-caae-406e-b63f-1683bffbdcfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 8.67 s, sys: 271 ms, total: 8.94 s\n",
            "Wall time: 8.94 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "file = open('glove.twitter.27B.50d.txt', encoding='utf-8')\n",
        "\n",
        "for line in file:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vectors = np.asarray(values[1: ])\n",
        "    glove_vectors[word] = vectors\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 511,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Rq9rCKqUCLc",
        "outputId": "d62d5694-70da-4ceb-b8ab-6e79a9c6e6c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1193514"
            ]
          },
          "execution_count": 511,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(glove_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ6Q70wWCZxn"
      },
      "source": [
        "### Creating Word Matrix For Each Word In Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 512,
      "metadata": {
        "id": "zzSy0pRaUEMl"
      },
      "outputs": [],
      "source": [
        "word_vector_matrix = np.zeros((vocab_size, 50))\n",
        "\n",
        "tokens = []\n",
        "labels = []\n",
        "\n",
        "for word, index in token.word_index.items():   # index returned here starts with 1 so we need set vocab_size = len(token.word_index) + 1  to be able to index up to the greatest token ID\n",
        "    vector = glove_vectors.get(word)\n",
        "    if vector is not None:\n",
        "        word_vector_matrix[index] = vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sJqShHXCZxn"
      },
      "source": [
        "### Word matrix size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 513,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4vxrz7eUK5D",
        "outputId": "35b2800c-88eb-403c-cd9a-53c841e22214"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9246, 50)"
            ]
          },
          "execution_count": 513,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_vector_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu4hKxBkCZxn"
      },
      "source": [
        "### Building The Glove Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 514,
      "metadata": {
        "id": "yd6McRJxCZxo"
      },
      "outputs": [],
      "source": [
        "# text model\n",
        "input1 = Input(shape = (max_length))\n",
        "embedding = Embedding(vocab_size, 50, weights = [word_vector_matrix], trainable = False, name='embedding')(input1)\n",
        "\n",
        "layer1 = Conv1D(64, 8, activation = 'relu')(embedding)\n",
        "layer2 = MaxPooling1D(2)(layer1)\n",
        "layer3 = Conv1D(32, 4, activation = 'relu')(layer2)\n",
        "layer4 = MaxPooling1D(2)(layer3)\n",
        "\n",
        "flat1 = Flatten()(layer4)\n",
        "dense1 = Dense(128, activation='relu')(flat1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CNN model for images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qopt0V3ECZxo"
      },
      "source": [
        "### Converting target label to list for labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 515,
      "metadata": {
        "id": "MNlqQpC4CZxo"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_labels_train = y_train.tolist()\n",
        "y_labels_validate = y_validate.tolist()\n",
        "y_labels_test = y_test.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKN9WKV6CZxo"
      },
      "source": [
        "### Creating dataset for our train/validation/test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 516,
      "metadata": {
        "id": "MHaSFFpSCZxo",
        "outputId": "20a41aae-fb78-4b0c-e9b9-d24eaf5af187"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4812 files belonging to 2 classes.\n",
            "Found 968 files belonging to 2 classes.\n",
            "Found 963 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "image_train = image_dataset_from_directory(\"images_train\", labels=y_labels_train, label_mode=\"binary\", image_size=(64,64), batch_size=32, color_mode='rgb')\n",
        "image_val = image_dataset_from_directory(\"images_validate\", labels=y_labels_validate, label_mode=\"binary\", image_size=(64,64), batch_size=32, color_mode='rgb')\n",
        "image_test = image_dataset_from_directory(\"images_test\", labels=y_labels_test, label_mode=\"binary\", image_size=(64,64), batch_size=32, color_mode='rgb')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfu_55JwCZxo"
      },
      "source": [
        "### Autotune with keras for performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 517,
      "metadata": {
        "id": "_ylnu_SkCZxo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-29 23:44:45.461120: W tensorflow/core/lib/png/png_io.cc:88] PNG warning: iCCP: known incorrect sRGB profile\n"
          ]
        }
      ],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_dataset = image_train.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# train data\n",
        "data_batches = []\n",
        "\n",
        "for batch in train_dataset:\n",
        "    data_batches.append(batch[0])\n",
        "\n",
        "image_data_train = np.concatenate(data_batches, axis=0)\n",
        "\n",
        "#validation data\n",
        "val_dataset = image_val.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "data_batches = []\n",
        "\n",
        "for batch in val_dataset:\n",
        "    try:\n",
        "        data_batches.append(batch[0])\n",
        "    except:\n",
        "        print(batch)\n",
        "\n",
        "image_data_validate = np.concatenate(data_batches, axis=0)\n",
        "\n",
        "# testd data\n",
        "test_dataset = image_test.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "data_batches = []\n",
        "\n",
        "for batch in test_dataset:\n",
        "    data_batches.append(batch[0])\n",
        "\n",
        "image_data_test = np.concatenate(data_batches, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 518,
      "metadata": {
        "id": "AQ6kMVcNCZxo",
        "outputId": "11c18381-3f52-4ffc-8081-bf8a174b26ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4812, 64, 64, 3)\n",
            "(968, 64, 64, 3)\n",
            "(963, 64, 64, 3)\n"
          ]
        }
      ],
      "source": [
        "print(image_data_train.shape)\n",
        "print(image_data_validate.shape)\n",
        "print(image_data_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiGAk7iyCZxp"
      },
      "source": [
        "### Building the CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 519,
      "metadata": {
        "id": "aq4DJKxLCZxp"
      },
      "outputs": [],
      "source": [
        "input2 = Input(shape=(64,64,3))\n",
        "rescaling = Rescaling(1./255)(input2) # scale pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 520,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv1 = Conv2D(64, (3, 3), activation='relu')(rescaling)\n",
        "pool1 = MaxPooling2D((2, 2), padding='same')(conv1)\n",
        "conv2 = Conv2D(32, (3, 3), activation='relu')(pool1)\n",
        "pool2 = MaxPooling2D((2, 2), padding='same')(conv2)\n",
        "flat2 = Flatten()(pool2)\n",
        "d_layer = Dense(128, activation='relu')(flat2)\n",
        "dense2 = Dense(32, activation='relu')(d_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEZzoCkmCZxp"
      },
      "source": [
        "## Creating a multimodal architecture by merging the dense layers of text and image models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 521,
      "metadata": {
        "id": "Ecu_H086CZxp",
        "outputId": "5f83fe5d-ac58-4974-81f2-b3ac74c061f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_27\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_84 (InputLayer)       [(None, 64, 64, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " input_83 (InputLayer)       [(None, 120)]                0         []                            \n",
            "                                                                                                  \n",
            " rescaling_18 (Rescaling)    (None, 64, 64, 3)            0         ['input_84[0][0]']            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 120, 50)              462300    ['input_83[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)          (None, 62, 62, 64)           1792      ['rescaling_18[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_24 (Conv1D)          (None, 113, 64)              25664     ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d_24 (MaxPooli  (None, 31, 31, 64)           0         ['conv2d_24[0][0]']           \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " max_pooling1d_24 (MaxPooli  (None, 56, 64)               0         ['conv1d_24[0][0]']           \n",
            " ng1D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)          (None, 29, 29, 32)           18464     ['max_pooling2d_24[0][0]']    \n",
            "                                                                                                  \n",
            " conv1d_25 (Conv1D)          (None, 53, 32)               8224      ['max_pooling1d_24[0][0]']    \n",
            "                                                                                                  \n",
            " max_pooling2d_25 (MaxPooli  (None, 15, 15, 32)           0         ['conv2d_25[0][0]']           \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " max_pooling1d_25 (MaxPooli  (None, 26, 32)               0         ['conv1d_25[0][0]']           \n",
            " ng1D)                                                                                            \n",
            "                                                                                                  \n",
            " flatten_57 (Flatten)        (None, 7200)                 0         ['max_pooling2d_25[0][0]']    \n",
            "                                                                                                  \n",
            " flatten_56 (Flatten)        (None, 832)                  0         ['max_pooling1d_25[0][0]']    \n",
            "                                                                                                  \n",
            " dense_138 (Dense)           (None, 128)                  921728    ['flatten_57[0][0]']          \n",
            "                                                                                                  \n",
            " dense_137 (Dense)           (None, 128)                  106624    ['flatten_56[0][0]']          \n",
            "                                                                                                  \n",
            " dense_139 (Dense)           (None, 32)                   4128      ['dense_138[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_27 (Concatenat  (None, 160)                  0         ['dense_137[0][0]',           \n",
            " e)                                                                  'dense_139[0][0]']           \n",
            "                                                                                                  \n",
            " dense_140 (Dense)           (None, 128)                  20608     ['concatenate_27[0][0]']      \n",
            "                                                                                                  \n",
            " dense_141 (Dense)           (None, 32)                   4128      ['dense_140[0][0]']           \n",
            "                                                                                                  \n",
            " dense_142 (Dense)           (None, 1)                    33        ['dense_141[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1573693 (6.00 MB)\n",
            "Trainable params: 1111393 (4.24 MB)\n",
            "Non-trainable params: 462300 (1.76 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "merge = concatenate([dense1, dense2])\n",
        "\n",
        "hidden1 = Dense(128, activation='relu')(merge)\n",
        "hidden2 = Dense(32, activation='relu')(hidden1)\n",
        "output = Dense(1, activation='sigmoid')(hidden2)\n",
        "\n",
        "model = Model(inputs=[input1, input2], outputs=output)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYMTxpNDCZxp"
      },
      "source": [
        "### Defining ModelCheckPoint & EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 522,
      "metadata": {
        "id": "83gaHhH9CZxp",
        "outputId": "8fa017d9-7179-41fe-ad2c-f02b2ea22b8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "151/151 [==============================] - ETA: 0s - loss: 0.5900 - accuracy: 0.6937\n",
            "Epoch 1: val_loss improved from inf to 0.59165, saving model to best_nultimodal.hdf5\n",
            "151/151 [==============================] - 20s 91ms/step - loss: 0.5900 - accuracy: 0.6937 - val_loss: 0.5917 - val_accuracy: 0.6818\n",
            "Epoch 2/50\n",
            "  4/151 [..............................] - ETA: 2s - loss: 0.5107 - accuracy: 0.7422"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/shashi/Downloads/testing/testenv/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "151/151 [==============================] - ETA: 0s - loss: 0.5918 - accuracy: 0.7066\n",
            "Epoch 2: val_loss did not improve from 0.59165\n",
            "151/151 [==============================] - 4s 26ms/step - loss: 0.5918 - accuracy: 0.7066 - val_loss: 0.6140 - val_accuracy: 0.6911\n",
            "Epoch 3/50\n",
            "151/151 [==============================] - ETA: 0s - loss: 0.5524 - accuracy: 0.7294\n",
            "Epoch 3: val_loss did not improve from 0.59165\n",
            "151/151 [==============================] - 3s 23ms/step - loss: 0.5524 - accuracy: 0.7294 - val_loss: 0.7465 - val_accuracy: 0.6539\n",
            "Epoch 4/50\n",
            "151/151 [==============================] - ETA: 0s - loss: 0.8774 - accuracy: 0.6964\n",
            "Epoch 4: val_loss did not improve from 0.59165\n",
            "151/151 [==============================] - 4s 24ms/step - loss: 0.8774 - accuracy: 0.6964 - val_loss: 0.9779 - val_accuracy: 0.5992\n",
            "Epoch 5/50\n",
            "151/151 [==============================] - ETA: 0s - loss: 1.0231 - accuracy: 0.6629\n",
            "Epoch 5: val_loss did not improve from 0.59165\n",
            "151/151 [==============================] - 4s 23ms/step - loss: 1.0231 - accuracy: 0.6629 - val_loss: 1.1044 - val_accuracy: 0.7066\n",
            "Epoch 6/50\n",
            "151/151 [==============================] - ETA: 0s - loss: 1.4592 - accuracy: 0.6949\n",
            "Epoch 6: val_loss did not improve from 0.59165\n",
            "151/151 [==============================] - 3s 23ms/step - loss: 1.4592 - accuracy: 0.6949 - val_loss: 1.3273 - val_accuracy: 0.6839\n",
            "Epoch 6: early stopping\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x31b99df10>"
            ]
          },
          "execution_count": 522,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath=\"best_nultimodal.hdf5\", verbose=2, save_best_only=True, monitor='val_loss')\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "model.fit([text_train, image_data_train], y_train, epochs=50, callbacks=[monitor, checkpointer], batch_size = 32,\n",
        "            validation_data=([text_validate, image_data_validate], y_validate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predicting with test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 523,
      "metadata": {
        "id": "wpWCOSg2CZxq",
        "outputId": "6a6e4ef9-482a-449b-9064-ecccf778d390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 3s 43ms/step\n"
          ]
        }
      ],
      "source": [
        "model.load_weights(\"best_nultimodal.hdf5\")\n",
        "pred = model.predict([text_test, image_data_test])\n",
        "\n",
        "pred = (pred > 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 524,
      "metadata": {
        "id": "fwUO7G7uCZxq",
        "outputId": "0de65e4d-a401-4d2f-f41c-545cf099bec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.81      0.78       571\n",
            "           1       0.69      0.61      0.65       392\n",
            "\n",
            "    accuracy                           0.73       963\n",
            "   macro avg       0.72      0.71      0.71       963\n",
            "weighted avg       0.73      0.73      0.73       963\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Feature - VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 525,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_85 (InputLayer)       [(None, 64, 64, 3)]       0         \n",
            "                                                                 \n",
            " rescaling_19 (Rescaling)    (None, 64, 64, 3)         0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 64, 64, 64)        1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 64, 64, 64)        36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 32, 32, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 32, 32, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 16, 16, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 16, 16, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14714688 (56.13 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "# Loading VGG16 model\n",
        "\n",
        "input_vgg = Input(shape=(64,64,3))\n",
        "rescaling = Rescaling(1./255)(input_vgg) # scale pixels\n",
        "\n",
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_tensor=rescaling)\n",
        "base_model.trainable = False\n",
        "\n",
        "base_model.summary()\n",
        "last_layer_vgg = base_model.layers[-1].output\n",
        "\n",
        "flatten_layer = Flatten()(last_layer_vgg)\n",
        "dense2 = Dense(128, activation='relu')(flatten_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 526,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_28\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_85 (InputLayer)       [(None, 64, 64, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " rescaling_19 (Rescaling)    (None, 64, 64, 3)            0         ['input_85[0][0]']            \n",
            "                                                                                                  \n",
            " block1_conv1 (Conv2D)       (None, 64, 64, 64)           1792      ['rescaling_19[0][0]']        \n",
            "                                                                                                  \n",
            " block1_conv2 (Conv2D)       (None, 64, 64, 64)           36928     ['block1_conv1[0][0]']        \n",
            "                                                                                                  \n",
            " block1_pool (MaxPooling2D)  (None, 32, 32, 64)           0         ['block1_conv2[0][0]']        \n",
            "                                                                                                  \n",
            " block2_conv1 (Conv2D)       (None, 32, 32, 128)          73856     ['block1_pool[0][0]']         \n",
            "                                                                                                  \n",
            " block2_conv2 (Conv2D)       (None, 32, 32, 128)          147584    ['block2_conv1[0][0]']        \n",
            "                                                                                                  \n",
            " block2_pool (MaxPooling2D)  (None, 16, 16, 128)          0         ['block2_conv2[0][0]']        \n",
            "                                                                                                  \n",
            " block3_conv1 (Conv2D)       (None, 16, 16, 256)          295168    ['block2_pool[0][0]']         \n",
            "                                                                                                  \n",
            " block3_conv2 (Conv2D)       (None, 16, 16, 256)          590080    ['block3_conv1[0][0]']        \n",
            "                                                                                                  \n",
            " block3_conv3 (Conv2D)       (None, 16, 16, 256)          590080    ['block3_conv2[0][0]']        \n",
            "                                                                                                  \n",
            " block3_pool (MaxPooling2D)  (None, 8, 8, 256)            0         ['block3_conv3[0][0]']        \n",
            "                                                                                                  \n",
            " block4_conv1 (Conv2D)       (None, 8, 8, 512)            1180160   ['block3_pool[0][0]']         \n",
            "                                                                                                  \n",
            " block4_conv2 (Conv2D)       (None, 8, 8, 512)            2359808   ['block4_conv1[0][0]']        \n",
            "                                                                                                  \n",
            " input_83 (InputLayer)       [(None, 120)]                0         []                            \n",
            "                                                                                                  \n",
            " block4_conv3 (Conv2D)       (None, 8, 8, 512)            2359808   ['block4_conv2[0][0]']        \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 120, 50)              462300    ['input_83[0][0]']            \n",
            "                                                                                                  \n",
            " block4_pool (MaxPooling2D)  (None, 4, 4, 512)            0         ['block4_conv3[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_24 (Conv1D)          (None, 113, 64)              25664     ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " block5_conv1 (Conv2D)       (None, 4, 4, 512)            2359808   ['block4_pool[0][0]']         \n",
            "                                                                                                  \n",
            " max_pooling1d_24 (MaxPooli  (None, 56, 64)               0         ['conv1d_24[0][0]']           \n",
            " ng1D)                                                                                            \n",
            "                                                                                                  \n",
            " block5_conv2 (Conv2D)       (None, 4, 4, 512)            2359808   ['block5_conv1[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_25 (Conv1D)          (None, 53, 32)               8224      ['max_pooling1d_24[0][0]']    \n",
            "                                                                                                  \n",
            " block5_conv3 (Conv2D)       (None, 4, 4, 512)            2359808   ['block5_conv2[0][0]']        \n",
            "                                                                                                  \n",
            " max_pooling1d_25 (MaxPooli  (None, 26, 32)               0         ['conv1d_25[0][0]']           \n",
            " ng1D)                                                                                            \n",
            "                                                                                                  \n",
            " block5_pool (MaxPooling2D)  (None, 2, 2, 512)            0         ['block5_conv3[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_56 (Flatten)        (None, 832)                  0         ['max_pooling1d_25[0][0]']    \n",
            "                                                                                                  \n",
            " flatten_58 (Flatten)        (None, 2048)                 0         ['block5_pool[0][0]']         \n",
            "                                                                                                  \n",
            " dense_137 (Dense)           (None, 128)                  106624    ['flatten_56[0][0]']          \n",
            "                                                                                                  \n",
            " dense_143 (Dense)           (None, 128)                  262272    ['flatten_58[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_28 (Concatenat  (None, 256)                  0         ['dense_137[0][0]',           \n",
            " e)                                                                  'dense_143[0][0]']           \n",
            "                                                                                                  \n",
            " dense_144 (Dense)           (None, 128)                  32896     ['concatenate_28[0][0]']      \n",
            "                                                                                                  \n",
            " dense_145 (Dense)           (None, 32)                   4128      ['dense_144[0][0]']           \n",
            "                                                                                                  \n",
            " dense_146 (Dense)           (None, 1)                    33        ['dense_145[0][0]']           \n",
            "                                                                                                  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================================================================\n",
            "Total params: 15616829 (59.57 MB)\n",
            "Trainable params: 439841 (1.68 MB)\n",
            "Non-trainable params: 15176988 (57.90 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "merge = concatenate([dense1, dense2])\n",
        "\n",
        "hidden1 = Dense(128, activation='relu')(merge)\n",
        "hidden2 = Dense(32, activation='relu')(hidden1)\n",
        "output = Dense(1, activation='sigmoid')(hidden2)\n",
        "\n",
        "\n",
        "model = Model(inputs=[input1, input_vgg], outputs=output)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 527,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "151/151 [==============================] - ETA: 0s - loss: 0.5882 - accuracy: 0.7105\n",
            "Epoch 1: val_loss improved from inf to 0.58459, saving model to best_nultimodal_with_vgg.hdf5\n",
            "151/151 [==============================] - 23s 108ms/step - loss: 0.5882 - accuracy: 0.7105 - val_loss: 0.5846 - val_accuracy: 0.6994\n",
            "Epoch 2/50\n",
            "  3/151 [..............................] - ETA: 4s - loss: 0.4767 - accuracy: 0.7188"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/shashi/Downloads/testing/testenv/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "151/151 [==============================] - ETA: 0s - loss: 0.4978 - accuracy: 0.7635\n",
            "Epoch 2: val_loss did not improve from 0.58459\n",
            "151/151 [==============================] - 5s 33ms/step - loss: 0.4978 - accuracy: 0.7635 - val_loss: 0.6071 - val_accuracy: 0.6860\n",
            "Epoch 3/50\n",
            "151/151 [==============================] - ETA: 0s - loss: 0.4285 - accuracy: 0.8078\n",
            "Epoch 3: val_loss did not improve from 0.58459\n",
            "151/151 [==============================] - 5s 32ms/step - loss: 0.4285 - accuracy: 0.8078 - val_loss: 0.6894 - val_accuracy: 0.7004\n",
            "Epoch 4/50\n",
            "151/151 [==============================] - ETA: 0s - loss: 0.3074 - accuracy: 0.8705\n",
            "Epoch 4: val_loss did not improve from 0.58459\n",
            "151/151 [==============================] - 5s 32ms/step - loss: 0.3074 - accuracy: 0.8705 - val_loss: 0.8482 - val_accuracy: 0.6787\n",
            "Epoch 5/50\n",
            "149/151 [============================>.] - ETA: 0s - loss: 0.2085 - accuracy: 0.9197\n",
            "Epoch 5: val_loss did not improve from 0.58459\n",
            "151/151 [==============================] - 5s 32ms/step - loss: 0.2080 - accuracy: 0.9198 - val_loss: 0.7225 - val_accuracy: 0.7324\n",
            "Epoch 6/50\n",
            "149/151 [============================>.] - ETA: 0s - loss: 0.1084 - accuracy: 0.9597\n",
            "Epoch 6: val_loss did not improve from 0.58459\n",
            "151/151 [==============================] - 5s 32ms/step - loss: 0.1101 - accuracy: 0.9591 - val_loss: 1.0974 - val_accuracy: 0.7159\n",
            "Epoch 6: early stopping\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7936a5ad0>"
            ]
          },
          "execution_count": 527,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# checkpointer = ModelCheckpoint(filepath=\"best_nultimodal_with_vgg.hdf5\", verbose=2, save_best_only=True, monitor='val_loss')\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "model.fit([text_train, image_data_train], y_train, epochs=50, callbacks=[monitor], batch_size = 32,\n",
        "            validation_data=([text_validate, image_data_validate], y_validate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 528,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 4s 89ms/step\n"
          ]
        }
      ],
      "source": [
        "# model.load_weights(\"best_nultimodal_with_vgg.hdf5\")\n",
        "pred = model.predict([text_test, image_data_test])\n",
        "pred = (pred > 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 529,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.79      0.77       571\n",
            "           1       0.67      0.63      0.65       392\n",
            "\n",
            "    accuracy                           0.72       963\n",
            "   macro avg       0.71      0.71      0.71       963\n",
            "weighted avg       0.72      0.72      0.72       963\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.16 ('csc219')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "46063a162fd018717c54ebe31a63604c323942fa1bfed7ede4efd019204c37b5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
